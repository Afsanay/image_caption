import os

import streamlit as st
import pickle
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Model
from model import load_model, predict_caption
import tensorflow as tf

tf.config.set_visible_devices([], 'GPU')


st.set_page_config(layout="wide")
with st.sidebar:
    st.image(
        'https://www.onepointltd.com/wp-content/uploads/2020/03/inno2.png')
    st.title("VGG16 with LSTM layer")
    st.info(
        "This is a model that can help in generating artificial captions for the images provided to it.")

st.title("Image Captioning with VGG and LSTM")

tab1,tab2 = st.tabs(["Demonstration","About"])

with tab1:
    vgg_model = VGG16()
    vgg_model = Model(inputs=vgg_model.inputs, outputs=vgg_model.layers[-2].output)

    tokenizer = Tokenizer()
    with open('tokenizer.pkl', 'rb') as f:
        tokenizer = pickle.load(f)
    vocab_size = len(tokenizer.word_index) + 1

    max_length = 35

    model = load_model()

    options = os.listdir(os.path.join('data', 'Images'))
    selected_image = st.selectbox('Choose an image', options=options)

    image_path = os.path.join('data', 'Images', selected_image)

    if image_path is not None:
        image = load_img(image_path, target_size=(224, 224))
        image = img_to_array(image)
        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))
        image = preprocess_input(image)
        feature = vgg_model.predict(image, verbose=0)
        generated = predict_caption(model, feature, tokenizer, max_length)


        col1,col2 = st.columns(2)

        with col1:
            st.info("The image chosen for captioning")
            st.image(image_path)

        with col2:
            st.info("The feature vector for the image generated by VGG16 model")
            st.text(feature)
            st.info("The Generated Caption")
            gen_words = generated.split(' ')
            generated = " ".join(gen_words[1:-1])
            st.text(generated)


with tab2:
    st.markdown("""
                <p style="font-size:20px; text-align:center">The VGG16 + LSTM architecture is a combination of two popular neural network components: the VGG16 convolutional neural network (CNN) and a Long Short-Term Memory (LSTM) recurrent neural network. This architecture is typically used for tasks that involve sequential or temporal data, such as video analysis, natural language processing, or time series prediction. The VGG16 network is pretrained on large-scale image classification tasks and has been proven to be effective in extracting high-level visual features from images.The LSTM, on the other hand, is a type of recurrent neural network (RNN) that is well-suited for handling sequential data. In the VGG16 + LSTM architecture, the VGG16 network is typically used as a feature extractor. These features are then fed into the LSTM network, which processes them sequentially, taking into account the temporal dependencies between the features.

</p>
                <img style="display: block; margin-left: auto; margin-right: auto;padding: 5px;" src="https://www.researchgate.net/publication/353925876/figure/fig6/AS:1057413557927937@1629118446583/Hybrid-model-VGG16-LSTM-Column-A-represents-the-sequence-of-images-extracted-from.ppm" />
                """,unsafe_allow_html=True)